---
layout: single
title:  "Blog @ Trustworthy AI"
permalink: /blogs/trustworthy-ai
---

## Survey

<!-- ### Trustworthy AI Description -->

- [x] [arXiv 2025] **AI Safety vs. AI Security: Demystifying the Distinction and Boundaries.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2506.18932) [![](https://img.shields.io/badge/slides-E29135)](https://zhiqlin.github.io/file/talks/AI_Safety_Security_July_17_2025.pdf) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/7k6RR4BMl7gcROFfzWhJjg)

<div align="center">
  <img src="./images/trustworthy-ai-demystify.png" alt="Trustworthy AI Framework" width="20%"/>
  <p style="font-family: 'Times New Roman';">AI Safety vs. AI Security</p>
</div>

Building trustworthy AI requires addressing both AI safety and AI security concerns. AI safety focuses on preventing unintended failures, including system malfunctions, design limitations, and unexpected behaviors that can arise during normal operation. In contrast, AI security addresses malicious exploitation threats, such as deliberate system manipulation, data poisoning attacks, and adversarial perturbations designed to compromise AI systems.


- [ ] [arXiv 2025] **A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2504.15585) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/ym-Bv1tPs57Y3zI-Pu6lzA)


<div style="display: flex; justify-content: center; align-items: flex-start; gap: 20px; flex-wrap: wrap;">
  <div style="text-align: center; width: 30%;">
    <img src="./images/trustworthy-ai-full-stack-safety.png" alt="Full Stack Safety" style="width: 100%;"/>
    <p style="font-family: 'Times New Roman';">Full Stack Safety</p>
  </div>
  <div style="text-align: center; width: 30%;">
    <img src="./images/trustworthy-ai-agent-system.png" alt="Agent System" style="width: 100%;"/>
    <p style="font-family: 'Times New Roman';">Agent System</p>
  </div>
  <div style="text-align: center; width: 30%;">
    <img src="./images/trustworthy-ai-agent-safety.png" alt="Agent Safety" style="width: 100%;"/>
    <p style="font-family: 'Times New Roman';">Agent Safety</p>
  </div>
</div>


This work defines the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment, and final commercialization. This comprehensive survey is grounded in an exhaustive review of over 900 papers and provides valuable guidance for researchers pursuing future work in this field.


- [ ] [arXiv 2025] **A Survey on AgentOps: Catagorization, Challenges, and Future Directions.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://www.arxiv.org/abs/2508.02121) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/UWR5BFKhJj0zrvjhjSymgQ)



<div align="center">
  <img src="./images/trustworthy-ai-agent-anomalies.png" alt="Anomalies in agent systems" width="30%"/>
  <p style="font-family: 'Times New Roman';">Agent Anomalies Detection</p>
</div>

This work conducts a comprehensive survey on agent system operations, establishing a clear framework for the field and defining key challenges to facilitate future development. The study systematically categorizes anomalies within agent systems into intra-agent and inter-agent anomalies, and introduces a novel operational framework for agent systems.


- [ ] [arXiv 2025] **Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2502.05206) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/CKVe-45__NFey16gex55zQ) [![](https://img.shields.io/badge/homepage-808080)](https://github.com/xingjunm/Awesome-Large-Model-Safety?tab=readme-ov-file) 


## AI Safety
### Alignment

- [ ] [arXiv 2025] **STAIR: Improving Safety Alignment with Introspective Reasoning.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2502.02384) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/1v4A6JBDSTrcw1nGnRR4ow) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/thu-ml/STAIR)



- [ ] [arXiv 2025] **RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2504.10081) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/1v4A6JBDSTrcw1nGnRR4ow) [![](https://img.shields.io/badge/code-B5739D)](https://huggingface.co/RealSafe)



## AI Security
### Jailbreak Attacks


- [ ] [NeurIPS 2024] **Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://proceedings.neurips.cc/paper_files/paper/2024/file/38c1dfb4f7625907b15e9515365e7803-Paper-Datasets_and_Benchmarks_Track.pdf) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/KulCxJm1wgz2fqorfuJ3Iw) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/usail-hkust/JailTrickBench.git)

<div align="center">
  <img src="./images/trustworthy-ai-jailbreak-JailTrickBench.png" alt="Anomalies in agent systems" width="30%"/>
  <p style="font-family: 'Times New Roman'">Benchmarking jailbreak attacks on LLMs</p>
</div>


- [ ] [ACL 2025] **Jailbreak Large Vision-Language Models Through Multi-Modal Linkage.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://aclanthology.org/2025.acl-long.74.pdf)


- [ ] [arXiv 2024] **Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2412.05934) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/XqsQE_tdA4gmlsDjp4pHtA)


- [ ] [arXiv 2025] **Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2505.16446v1)





